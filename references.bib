@inproceedings{ioffe2015batchnorm,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{dosreis2016transreal,
  title={Transreal calculus},
  author={dos Reis, Tiago S and Anderson, James AD},
  journal={IAENG International Journal of Applied Mathematics},
  volume={46},
  number={1},
  pages={15--26},
  year={2016}
}


@inproceedings{kingma2015adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@article{ba2016layernorm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

% ========== RATIONAL NEURAL NETWORKS ==========

@article{boulle2020rational,
  title={Rational neural networks},
  author={Boulle, Nicolas and Nakatsukasa, Yuji and Townsend, Alex},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14243--14253},
  year={2020}
}

@article{telgarsky2017neural,
  title={Neural networks and rational functions},
  author={Telgarsky, Matus},
  journal={International Conference on Machine Learning},
  pages={3387--3393},
  year={2017},
  organization={PMLR}
}

@book{baker1996pade,
  title={Pad{\'e} approximants},
  author={Baker Jr, George A and Graves-Morris, Peter},
  year={1996},
  publisher={Cambridge University Press}
}

@article{montanher2020pade,
  title={Pad{\'e} activation units: End-to-end learning of flexible activation functions in deep networks},
  author={Molina, Alejandro and Schramowski, Patrick and Kersting, Kristian},
  journal={arXiv preprint arXiv:1907.06732},
  year={2019}
}


% ========== ROBOTICS SINGULARITIES ==========

@book{nakamura1991advanced,
  title={Advanced robotics: redundancy and optimization},
  author={Nakamura, Yoshihiko},
  year={1991},
  publisher={Addison-Wesley}
}




@article{marquardt1963algorithm,
  title={An algorithm for least-squares estimation of nonlinear parameters},
  author={Marquardt, Donald W},
  journal={Journal of the society for Industrial and Applied Mathematics},
  volume={11},
  number={2},
  pages={431--441},
  year={1963},
  publisher={SIAM}
}

@book{siciliano2016robotics,
  title={Robotics: modelling, planning and control},
  author={Siciliano, Bruno and Sciavicco, Lorenzo and Villani, Luigi and Oriolo, Giuseppe},
  year={2016},
  publisher={Springer}
}


% ========== NUMERICAL STABILITY & ANALYSIS ==========

@book{higham2002accuracy,
  title={Accuracy and stability of numerical algorithms},
  author={Higham, Nicholas J},
  year={2002},
  publisher={SIAM}
}

@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}

@book{muller2010handbook,
  title={Handbook of Floating-Point Arithmetic},
  author={Muller, Jean-Michel and Brisebarre, Nicolas and De Dinechin, Florent and Jeannerod, Claude-Pierre and Lefevre, Vincent and Melquiond, Guillaume and Revol, Nathalie and Stehl{\'e}, Damien and Torres, Serge},
  volume={1},
  address={Basel, Switzerland},
  publisher={Birkh\"{a}user},
  year={2018}
}

@book{trefethen1997numerical,
  title={Numerical linear algebra},
  author={Trefethen, Lloyd N. and Bau, David},
  year={2022},
  publisher={Society for Industrial and Applied Mathematics}
}

@article{shewchuk1997adaptive,
  title={Adaptive precision floating-point arithmetic and fast robust geometric predicates},
  author={Shewchuk, Jonathan Richard},
  journal={Discrete \& computational geometry},
  volume={18},
  number={3},
  pages={305--363},
  year={1997},
  publisher={Springer}
}

% ========== IEEE-754 STANDARD ==========

@techreport{ieee754-2019,
  title={{IEEE} Standard for Floating-Point Arithmetic},
  author={{IEEE Computer Society}},
  institution={Institute of Electrical and Electronics Engineers},
  number={IEEE Std 754-2019},
  year={2019}
}

@article{goldberg1991every,
  title={What every computer scientist should know about floating-point arithmetic},
  author={Goldberg, David},
  journal={ACM computing surveys},
  volume={23},
  number={1},
  pages={5--48},
  year={1991},
  publisher={ACM}
}

@article{kahan1996ieee,
  title={IEEE standard 754 for binary floating-point arithmetic},
  author={Kahan, William},
  journal={Lecture notes on the status of IEEE},
  volume={754},
  year={1996}
}

% ========== TRANSREAL ARITHMETIC ==========

@inproceedings{anderson2006perspex,
  title={Perspex Machine VIII: Axioms of Transreal Arithmetic},
  author={Anderson, J. A. D. W. and VÃ¶lker, Norbert and Adams, Andrew A.},
  booktitle={Vision Geometry XV},
  series={Proceedings of SPIE},
  volume={6499},
  pages={649902-1--649902-12},
  year={2007},
  publisher={SPIE}
}

% Removed anderson2019transmathematics per revision

@article{reis2016transreal,
  title={Transreal calculus},
  author={dos Reis, Tiago S and Anderson, James AD},
  journal={IAENG International Journal of Applied Mathematics},
  volume={46},
  number={1},
  pages={1--26},
  year={2016}
}

@article{reis2016transfields,
  title={Construction of the transreal numbers and algebraic transfields},
  author={dos Reis, Tiago S and Gomide, Walter and Anderson, J. A. D. W.},
  journal={IAENG International Journal of Applied Mathematics},
  volume={46},
  number={1},
  pages={11--23},
  year={2016}
}

% ========== LEARNING WITH DISCONTINUITIES ==========

@article{sitzmann2020implicit,
  title={Implicit neural representations with periodic activation functions},
  author={Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7462--7473},
  year={2020}
}

@article{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  journal={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}

@article{wang2021understanding,
  title={Understanding and mitigating gradient flow pathologies in physics-informed neural networks},
  author={Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3055--A3081},
  year={2021},
  publisher={SIAM}
}

@article{krishnapriyan2021characterizing,
  title={Characterizing possible failure modes in physics-informed neural networks},
  author={Krishnapriyan, Aditi and Gholami, Amir and Zhe, Shandian and Kirby, Robert and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26548--26560},
  year={2021}
}

@article{jagtap2020conservative,
  title={Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  author={Jagtap, Ameya D and Kawaguchi, Kenji and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={365},
  pages={113028},
  year={2020},
  publisher={Elsevier}
}

% ========== GRADIENT METHODS & OPTIMIZATION ==========

@article{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  journal={International conference on machine learning},
  pages={1310--1318},
  year={2013},
  organization={PMLR}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

% ========== AUTOMATIC DIFFERENTIATION ==========

@book{griewank2008evaluating,
  title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  year={2008},
  publisher={SIAM}
}

@article{baydin2017automatic,
  title={Automatic differentiation in machine learning: a survey},
  author={Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  journal={Journal of machine learning research},
  volume={18},
  year={2017}
}

% ========== UNIVERSAL APPROXIMATION ==========

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@article{pinkus1999approximation,
  title={Approximation theory of the MLP model in neural networks},
  author={Pinkus, Allan},
  journal={Acta numerica},
  volume={8},
  pages={143--195},
  year={1999},
  publisher={Cambridge University Press}
}
