\\documentclass{article}
\\usepackage{tmlr}
\\usepackage{amsmath, amssymb, amsthm}
\\usepackage{graphicx}
\\usepackage{booktabs}
\\usepackage{natbib}

% theorem environments
\\newtheorem{theorem}{Theorem}
\\newtheorem{lemma}{Lemma}
\\newtheorem{proposition}{Proposition}
\\newtheorem{definition}{Definition}

% macros
\\newcommand{\\tr}{\\text{TR}}
\\newcommand{\\wheel}{\\text{Wheel}}
\\newcommand{\\real}{\\mathbb{R}}
\\newcommand{\\phiTag}{\\Phi}

\\title{ZeroProofML: Epsilon-Free Rational Neural Layers via Transreal Arithmetic}

\\author{Anonymous Authors}

\\begin{document}

\\maketitle

\\begin{abstract}
We introduce ZeroProofML, a framework for epsilon-free rational neural layers based on transreal arithmetic. By totalizing division and other singular operations, our approach eliminates the need for $\\varepsilon$ hacks, achieves stability near poles, and provides deterministic semantics. We formalize transreal autodiff, introduce TR-Norm (an epsilon-free normalization), prove stability bounds, and validate empirically against rational and polynomial baselines. Our results suggest ZeroProofML offers a robust, reproducible alternative to conventional techniques near singularities.
\\end{abstract}

\\section{Introduction}
At the heart of modern machine learning lies a paradox: our models are built on mathematical foundations that forbid division by zero, yet in practice they rely on ad-hoc numerical “fixes” to avoid it. The most common of these, the **$\\varepsilon$-trick**, replaces unstable denominators with $Q(x)+\\varepsilon$ for small $\\varepsilon$ \\citep{ioffe2015batchnorm,boulle2020rational}. This pragmatic adjustment enables training but breaks mathematical consistency, obscures theoretical guarantees, and compromises reproducibility \\citep{henderson2018rlmatters}. As models scale and are deployed in safety-critical settings, reliance on $\\varepsilon$-hacks becomes increasingly untenable.

This paper develops a principled alternative: **ZeroProofML**, a framework that replaces $\\varepsilon$-hacks with **transreal arithmetic (TR)**. TR extends the real numbers with totalized operations, adding well-defined outcomes for division by zero: $+\\infty$, $-\\infty$, and nullity $\\Phi$ for indeterminate forms \\citep{anderson2019transmathematics,dosreis2016transreal}. Building on this foundation, we construct **rational neural layers** of the form $P(x)/Q(x)$ that remain mathematically total and algorithmically stable.

The central idea is to carry *tags* (REAL, $\\pm\\infty$, $\\Phi$) alongside values. **TR-Autodiff** enforces the *Mask-REAL* rule: on REAL paths, gradients coincide with classical calculus; on non-REAL paths, gradients vanish exactly, preventing instabilities \\citep{baydin2018autodiff}. **TR-Norm** provides an $\\varepsilon$-free batch normalization, with a deterministic bypass when variance is zero. Together, these components form an ML stack where every operation is total, deterministic, and reproducible by construction.

We prove that ZeroProofML layers admit bounded updates near poles, are identifiable without $\\varepsilon$, and converge stably under standard optimization. Empirically, we demonstrate improved stability and extrapolation compared to $\\varepsilon$-rational baselines. ZeroProofML thus offers not just a new layer design, but a new paradigm: machine learning without $\\varepsilon$-hacks, grounded in total arithmetic and reproducible semantics.

\\section{Method}

We now introduce the formal machinery underlying **ZeroProofML**. Our method is based on **transreal arithmetic (TR)**, which extends the reals to a *total algebraic structure* \\citep{anderson2019transmathematics}.

\\subsection{Transreal Carrier and Tags}
We define the scalar carrier as
\\[
TR = \\{ (v, t) \\mid v \\in \\mathbb{R} \\cup \\{\\text{NaN}\\}, \\; t \\in \\{\\text{REAL}, \\text{PINF}, \\text{NINF}, \\Phi\\}\\}.
\\]
- REAL: finite values.  \\
- PINF / NINF: $+\\infty$ and $-\\infty$.  \\
- $\\Phi$: nullity, representing indeterminate forms such as $0/0$, $\\infty-\\infty$, or $0 \\cdot \\infty$.  

Closed operation tables for $+, \\times, \\div$ guarantee **totality**: every expression evaluates to a well-defined tagged value, never raising exceptions.

\\subsection{Totalized Operations}
Arithmetic follows TR rules \\citep{dosreis2016transreal,bergstra2021wheel}:
- Division: $\\tfrac{a}{0} =$ PINF/NINF if $a \\ne 0$, $\\Phi$ if $a=0$.  \\
- Multiplication: $0 \\times \\infty = \\Phi$, $\\infty \\times \\infty = \\infty$.  \\
- Addition: $\\infty + (-\\infty) = \\Phi$.  
These rules align with established transreal systems while preserving deterministic evaluation.

\\subsection{TR-Autodiff}
Autodiff is extended via the **Mask-REAL rule**:
- If a forward node tag is REAL, gradients coincide with classical derivatives.  \\
- If the node tag is non-REAL (PINF, NINF, $\\Phi$), **all parameter/input partials are zero**.  

\\textbf{Lemma (Mask-REAL Composition).} Any computational path passing through a non-REAL node contributes zero to the overall Jacobian.  \\
\\emph{Proof sketch.} Immediate by induction on path length and the chain rule, since all derivatives vanish at the first non-REAL node.

\\subsection{TR-Rational Layers}
We define rational layers \\citep{boulle2020rational}:
\\[
y(x) = \\frac{P_\\theta(x)}{Q_\\phi(x)}, \\quad 
Q_\\phi(x) = 1 + \\sum_{k=1}^{d_Q} \\phi_k \\psi_k(x),
\\]
with polynomial bases $\\psi_k$. Identifiability is ensured by fixing the leading term of $Q$. Forward semantics use TR rules; gradients use TR-AD with Mask-REAL.

\\subsection{TR-Norm}
For normalization, we compute feature-wise variance over REAL samples only.  \\
- If $\\sigma^2 > 0$: normalize as in batch normalization \\citep{ioffe2015batchnorm}.  \\
- If $\\sigma^2 = 0$: deterministically bypass, outputting $\\beta$.  

This ensures totality and limit-equivalence to BN as $\\varepsilon \\to 0^+$, without introducing $\\varepsilon$.

\\subsection{IEEE $\\leftrightarrow$ TR Bridge}
We provide a bidirectional mapping: finite floats $\\leftrightarrow$ REAL, NaN $\\leftrightarrow$ $\\Phi$, $\\pm\\infty$ $\\leftrightarrow$ PINF/NINF. Round-trips preserve semantics, ensuring framework compatibility.

