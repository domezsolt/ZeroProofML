---- adaptive_loss_demo ----
Adaptive Loss Policy Demonstration
==================================

Generating synthetic data with singularity at x=0.5...

Creating TR-Rational model with adaptive loss...

Training with target coverage: 0.9
Starting training for 50 epochs
Using adaptive loss with target coverage: 0.9
Adaptive Loss Policy Demonstration
==================================

Generating synthetic data with singularity at x=0.5...

Creating TR-Rational model with adaptive loss...

Training with target coverage: 0.9
Starting training for 50 epochs
Using adaptive loss with target coverage: 0.9
Epoch 1 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 1/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 2 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 2/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 3 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 3/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 4 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 4/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 5 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 5 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 5 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.995
Epoch 5 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.995
Epoch 5 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.986
Epoch 5/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.995 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 6 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.986
Epoch 6 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.972
Epoch 6 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.972
Epoch 6 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.955
Epoch 6 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.955
Epoch 6/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.966 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 7 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.934
Epoch 7 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.934
Epoch 7 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.911
Epoch 7 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.911
Epoch 7 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.885
Epoch 7/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.913 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 8 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.885
Epoch 8 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.856
Epoch 8 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.856
Epoch 8 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.826
Epoch 8 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.826
Epoch 8/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.846 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 9 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.793
Epoch 9 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.793
Epoch 9 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.759
Epoch 9 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.759
Epoch 9 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.723
Epoch 9/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.763 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 10 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.723
Epoch 10 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.686
Epoch 10 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.686
Epoch 10 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.647
Epoch 10 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.647
Epoch 10/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.673 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 11 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.607
Epoch 11 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.607
Epoch 11 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.567
Epoch 11 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.567
Epoch 11 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.525
Epoch 11/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.571 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 12 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.525
Epoch 12 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.482
Epoch 12 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.482
Epoch 12 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.439
Epoch 12 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.439
Epoch 12/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.468 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 13 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.395
Epoch 13 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.395
Epoch 13 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.351
Epoch 13 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.351
Epoch 13 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.306
Epoch 13/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.356 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 14 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.306
Epoch 14 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.260
Epoch 14 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.260
Epoch 14 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.214
Epoch 14 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.214
Epoch 14/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.245 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 15 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.168
Epoch 15 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.168
Epoch 15 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.121
Epoch 15 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.121
Epoch 15 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.074
Epoch 15/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.126 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 16 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.074
Epoch 16 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.026
Epoch 16 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.026
Epoch 16 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 16 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 16/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.022 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 17 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 17/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 18 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 18/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 19 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 19/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 20 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 20/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 21 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 21/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 22 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 22/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 23 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 23/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 24 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 24/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 25 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 25/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 26 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 26/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 27 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 27/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 28 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 28/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 29 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 29/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 30 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 30/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 31 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 31/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 32 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 32/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 33 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 33/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 34 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 34/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 35 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 35/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 36 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 36/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 37 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 37/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 38 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 38/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 39 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 39/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 40 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 40/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 41 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 41/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 42 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 42/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 43 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 43/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 44 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 44/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 45 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 45/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 46 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 46/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 47 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 47/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 48 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 48/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 49 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 49/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 50 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 50/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Training completed in 24.46 seconds

Evaluation Results:
Coverage: 1.000 (200/200 REAL)
MSE (REAL only): 2348.789768

Tag Distribution:
  REAL: 200 (100.0%)
  PINF: 0 (0.0%)
  NINF: 0 (0.0%)
  PHI: 0 (0.0%)
  BOTTOM: 0 (0.0%)

Final Adaptive Loss Statistics:
  Final λ_rej: 0.0000
  Final coverage: 1.000
  Coverage gap: -0.1000
  Total samples: 40000

---- adaptive_loss_demo ----
Adaptive Loss Policy Demonstration
==================================

Generating synthetic data with singularity at x=0.5...

Creating TR-Rational model with adaptive loss...

Training with target coverage: 0.9
Starting training for 50 epochs
Using adaptive loss with target coverage: 0.9
Epoch 1 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 1 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 1/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 2 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 2 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 2/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 3 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 3 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 3/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 4 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 1.000
Epoch 4 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 1.000
Epoch 4/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 1.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 5 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 1.000
Epoch 5 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 1.000
Epoch 5 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.995
Epoch 5 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.995
Epoch 5 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.986
Epoch 5/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.995 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 6 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.986
Epoch 6 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.972
Epoch 6 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.972
Epoch 6 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.955
Epoch 6 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.955
Epoch 6/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.966 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 7 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.934
Epoch 7 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.934
Epoch 7 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.911
Epoch 7 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.911
Epoch 7 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.885
Epoch 7/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.913 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 8 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.885
Epoch 8 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.856
Epoch 8 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.856
Epoch 8 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.826
Epoch 8 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.826
Epoch 8/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.846 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 9 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.793
Epoch 9 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.793
Epoch 9 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.759
Epoch 9 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.759
Epoch 9 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.723
Epoch 9/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.763 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 10 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.723
Epoch 10 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.686
Epoch 10 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.686
Epoch 10 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.647
Epoch 10 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.647
Epoch 10/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.673 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 11 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.607
Epoch 11 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.607
Epoch 11 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.567
Epoch 11 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.567
Epoch 11 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.525
Epoch 11/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.571 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 12 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.525
Epoch 12 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.482
Epoch 12 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.482
Epoch 12 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.439
Epoch 12 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.439
Epoch 12/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.468 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 13 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.395
Epoch 13 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.395
Epoch 13 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.351
Epoch 13 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.351
Epoch 13 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.306
Epoch 13/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.356 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 14 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.306
Epoch 14 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.260
Epoch 14 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.260
Epoch 14 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.214
Epoch 14 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.214
Epoch 14/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.245 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 15 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.168
Epoch 15 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.168
Epoch 15 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.121
Epoch 15 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.121
Epoch 15 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.074
Epoch 15/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.126 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 16 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.074
Epoch 16 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.026
Epoch 16 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.026
Epoch 16 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 16 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 16/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.022 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 17 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 17 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 17/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 18 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 18 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 18/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 19 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 19 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 19/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 20 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 20 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 20/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 21 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 21 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 21/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 22 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 22 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 22/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 23 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 23 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 23/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 24 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 24 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 24/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 25 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 25 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 25/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 26 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 26 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 26/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 27 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 27 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 27/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 28 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 28 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 28/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 29 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 29 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 29/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 30 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 30 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 30/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 31 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 31 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 31/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 32 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 32 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 32/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 33 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 33 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 33/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 34 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 34 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 34/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 35 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 35 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 35/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 36 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 36 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 36/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 37 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 37 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 37/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 38 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 38 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 38/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 39 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 39 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 39/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 40 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 40 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 40/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 41 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 41 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 41/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 42 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 42 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 42/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 43 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 43 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 43/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 44 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 44 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 44/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 45 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 45 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 45/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 46 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 46 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 46/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 47 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 47 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 47/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 48 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 48 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 48/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 49 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 49 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 49/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Epoch 50 [0/25] Loss: 977.2143 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [5/25] Loss: 50514.0663 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [10/25] Loss: 29450.8624 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [15/25] Loss: 4680.2965 Coverage: 1.000 λ_rej: 0.000
Epoch 50 [20/25] Loss: 2465.5572 Coverage: 1.000 λ_rej: 0.000
Epoch 50/50 - Train Loss: 52701.4549 Coverage: 1.000 λ_rej: 0.000 - Val Loss: 1054.0216 Coverage: 1.000
Training completed in 22.36 seconds

Evaluation Results:
Coverage: 1.000 (200/200 REAL)
MSE (REAL only): 2348.789768

Tag Distribution:
  REAL: 200 (100.0%)
  PINF: 0 (0.0%)
  NINF: 0 (0.0%)
  PHI: 0 (0.0%)
  BOTTOM: 0 (0.0%)

Final Adaptive Loss Statistics:
  Final λ_rej: 0.0000
  Final coverage: 1.000
  Coverage gap: -0.1000
  Total samples: 40000
---- coverage_control_demo ----
======================================================================
COVERAGE CONTROL DEMONSTRATION
======================================================================

1. Generating challenging data with 40% near-pole samples...
   Training samples: 300
   Test samples: 100

2. Training WITHOUT coverage control...
--------------------------------------------------

3. Training WITH coverage control (Lagrange)...
--------------------------------------------------

4. Training WITH coverage control (PID)...
--------------------------------------------------

5. COMPARISON OF RESULTS
======================================================================

Final Metrics:
--------------------------------------------------
Method               Loss         Coverage     Lambda      
--------------------------------------------------
No Control           244.281895   1.000        1.000       
Lagrange Control     244.281895   1.000        1.428       
PID Control          244.281895   1.000        0.000       

6. COVERAGE STABILITY
--------------------------------------------------
Method               Mean±Std        Min-Max         Final     
--------------------------------------------------
No Control           1.000±0.000    1.000-1.000    1.000
Lagrange Control     1.000±0.000    1.000-1.000    1.000
PID Control          1.000±0.000    1.000-1.000    1.000

7. COVERAGE ENFORCEMENT STATISTICS
--------------------------------------------------
Lagrange Controller:
  Lambda adjustments: 30
  Coverage violations: 0
  Interventions: 0
  Coverage restored at epoch: 1

PID Controller:
  Lambda adjustments: 30
  Coverage violations: 0
  Interventions: 0
  Coverage restored at epoch: 1

8. Visualizing Lagrange control dynamics...

Visualization saved as 'coverage_control_dynamics.png'

======================================================================
DEMONSTRATION COMPLETE
======================================================================

KEY INSIGHTS:
• Coverage control maintains desired REAL output percentage
• Adaptive lambda prevents trivial rejection of difficult samples
• Different strategies (Lagrange, PID) offer different convergence behaviors
• Interventions trigger when coverage drops critically low
• Model learns to handle near-pole samples rather than rejecting them

To explore further:
• Try different target_coverage values (0.7 to 0.95)
• Experiment with coverage_strategy (LAGRANGE, PID, ADAPTIVE_RATE)
• Enable oversample_near_pole for weighted sampling
• Combine with hybrid gradients and tag-loss for maximum benefit
---- hybrid_gradient_demo ----
======================================================================
HYBRID GRADIENT SCHEDULE DEMONSTRATION
======================================================================

1. Generating synthetic data with pole at x=0.5...
   Training samples: 150
   Test samples: 50

2. Creating rational models...

3. Training STANDARD model (Mask-REAL only)...
--------------------------------------------------

4. Training HYBRID model (with gradient schedule)...
--------------------------------------------------

5. COMPARISON OF RESULTS
======================================================================

Final Training Loss:
  Standard (Mask-REAL):  239.626144
  Hybrid Schedule:       239.626144
  Improvement:           0.0%

Final Coverage (% REAL outputs):
  Standard:              1.000
  Hybrid:                1.000

Hybrid Gradient Statistics:
  Mode: converged (delta=1.000e-04)
  Near-pole gradient calls: 0.0%
  Min |Q| observed: N/A

6. TEST SET EVALUATION
--------------------------------------------------

Test Loss (MSE on REAL outputs):
  Standard:              262.300452
  Hybrid:                262.300452

Test Coverage:
  Standard:              1.000
  Hybrid:                1.000

7. Generating visualization for hybrid model...

Plot saved as 'hybrid_gradient_progress.png'

======================================================================
DEMONSTRATION COMPLETE
======================================================================

KEY INSIGHTS:
• The hybrid schedule enables learning near poles without instability
• Warmup phase ensures stable initial training
• Transition phase gradually enables near-pole gradient flow
• Final performance shows improved accuracy near singularities

To explore further:
• Adjust hybrid_warmup_epochs and hybrid_transition_epochs
• Try different delta_init and delta_final values
• Experiment with ScheduleType.LINEAR or ScheduleType.COSINE
• Enable tag_loss and pole_head for advanced features

---- pole_detection_demo ----

=== Pole Detection Demo ===

Generated 200 training samples
Near-pole samples: 33/200

Model initialized with 123 parameters

Training with pole detection...
Epoch   0: Loss=7.4745, Pole Accuracy=40.00%
Epoch  10: Loss=7.4745, Pole Accuracy=40.00%
Epoch  20: Loss=7.4745, Pole Accuracy=40.00%
Epoch  30: Loss=7.4745, Pole Accuracy=40.00%
Epoch  40: Loss=7.4745, Pole Accuracy=40.00%

=== Training Complete ===

Evaluating pole detection on test set...

=== Summary Statistics ===
Final pole detection accuracy: 40.00%
Detected pole regions (P > 0.8): [np.float64(0.9090909090909092)]...
Distance to pole 1 (x=0.5): 0.409
Distance to pole 2 (x=-0.7): 1.609

=== Model Integration Summary ===
hybrid_enabled: False
tag_prediction_enabled: True
tag_accuracy: 1.0
pole_detection_enabled: True
q_min_observed: None
q_max_observed: None
total_parameters: 123

=== Comparison: With vs Without Pole Head ===

Training WITHOUT pole head...
Training WITH pole head...

Final loss without pole head: 6.0249
Final loss with pole head: 6.0405
Average error near poles without head: 11.089
Average error near poles with head: 11.089

=== Demo Complete ===
Results saved to:
  - pole_detection_results.png
  - pole_head_comparison.png
  
---- saturating_grad_demo ----
Saturating Gradient Mode Demonstration
======================================

Generated 98 training points with poles at x=0.3 and x=-0.7

Training with Mask-REAL mode...
Epoch 0: Loss=170.9427, Coverage=1.000, Avg Gradient Norm=13.0057
Epoch 20: Loss=170.8125, Coverage=1.000, Avg Gradient Norm=1.9520
Epoch 40: Loss=170.7486, Coverage=1.000, Avg Gradient Norm=1.8450
Epoch 60: Loss=170.7386, Coverage=1.000, Avg Gradient Norm=1.8286
Epoch 80: Loss=170.7266, Coverage=1.000, Avg Gradient Norm=1.8226

Training with Saturating mode (bound=1.0)...
Epoch 0: Loss=170.9424, Coverage=1.000, Avg Gradient Norm=13.0265
Epoch 20: Loss=170.8105, Coverage=1.000, Avg Gradient Norm=0.0082
Epoch 40: Loss=170.8105, Coverage=1.000, Avg Gradient Norm=0.0082
Epoch 60: Loss=170.8105, Coverage=1.000, Avg Gradient Norm=0.0082
Epoch 80: Loss=170.8105, Coverage=1.000, Avg Gradient Norm=0.0082

Training with Saturating mode (bound=5.0)...
Epoch 0: Loss=170.9379, Coverage=1.000, Avg Gradient Norm=13.0051
Epoch 20: Loss=170.7950, Coverage=1.000, Avg Gradient Norm=0.1377
Epoch 40: Loss=170.7941, Coverage=1.000, Avg Gradient Norm=0.1374
Epoch 60: Loss=170.7932, Coverage=1.000, Avg Gradient Norm=0.1370
Epoch 80: Loss=170.7923, Coverage=1.000, Avg Gradient Norm=0.1367

Final Statistics:
Mode                 Final Loss      Final Coverage  Avg Gradient   
-----------------------------------------------------------------
Mask-REAL            170.7152        1.000           1.8066         
Saturating (b=1)     170.8105        1.000           0.0082         
Saturating (b=5)     170.7915        1.000           0.1365         

Gradient Analysis Near Poles:

At x = 0.31:
  Mask-REAL: tag=REAL, grad_norm=0.1069
  Saturating (b=1): tag=REAL, grad_norm=0.0007
  Saturating (b=5): tag=REAL, grad_norm=0.0095

At x = 0.29:
  Mask-REAL: tag=REAL, grad_norm=0.1014
  Saturating (b=1): tag=REAL, grad_norm=0.0007
  Saturating (b=5): tag=REAL, grad_norm=0.0097

At x = -0.69:
  Mask-REAL: tag=REAL, grad_norm=0.2301
  Saturating (b=1): tag=REAL, grad_norm=0.0006
  Saturating (b=5): tag=REAL, grad_norm=0.0156

At x = -0.71:
  Mask-REAL: tag=REAL, grad_norm=0.2990
  Saturating (b=1): tag=REAL, grad_norm=0.0006
  Saturating (b=5): tag=REAL, grad_norm=0.0156
---- tag_loss_demo ----
======================================================================
TAG-LOSS DEMONSTRATION
======================================================================

1. Generating data with multiple singularity types...
   Training samples: 300
   Test samples: 100

2. Creating models...

3. Training WITHOUT tag-loss...
--------------------------------------------------

4. Training WITH tag-loss...
--------------------------------------------------

5. COMPARISON OF RESULTS
======================================================================

Final Training Loss:
  Without tag-loss:  0.406886
  With tag-loss:     0.406886

6. TAG PREDICTION ACCURACY
--------------------------------------------------

Overall tag accuracy: 1.000

Per-class accuracy:
  REAL  : 1.000 (100 samples)

7. COVERAGE ANALYSIS
--------------------------------------------------

Output tag distribution (test set):
  Without tag-loss:
    REAL      : 1.000
    PINF      : 0.000
    NINF      : 0.000
    PHI       : 0.000
  With tag-loss:
    REAL      : 1.000
    PINF      : 0.000
    NINF      : 0.000
    PHI       : 0.000

8. Generating visualization...

Visualization saved as 'tag_loss_visualization.png'

======================================================================
DEMONSTRATION COMPLETE
======================================================================

KEY INSIGHTS:
• Tag-loss enables non-REAL samples to contribute supervision
• Model learns to classify singularity types (PINF/NINF/PHI)
• Auxiliary loss improves overall training stability
• Tag prediction accuracy indicates understanding of pole geometry

To explore further:
• Adjust lambda_tag to control tag-loss weight
• Increase tag_head_hidden_dim for more complex classification
• Combine with hybrid gradient schedule for maximum benefit
• Use tag predictions for uncertainty estimation
