"""
Profiling tools for transreal computations.

This module provides profiling and performance analysis tools
for transreal arithmetic operations.
"""

import time
import functools
import threading
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import tracemalloc
import gc
import sys

from ..core import TRScalar, TRTag
from ..autodiff import TRNode, OpType

PROFILING_AVAILABLE = True


@dataclass
class ProfileResult:
    """Result of a profiling operation."""
    name: str
    duration: float  # seconds
    calls: int
    memory_allocated: int  # bytes
    memory_peak: int  # bytes
    tag_distribution: Dict[str, int] = field(default_factory=dict)
    sub_operations: List['ProfileResult'] = field(default_factory=list)
    
    @property
    def avg_duration(self) -> float:
        """Average duration per call."""
        return self.duration / self.calls if self.calls > 0 else 0.0
    
    @property
    def memory_allocated_mb(self) -> float:
        """Memory allocated in MB."""
        return self.memory_allocated / (1024 * 1024)
    
    @property
    def memory_peak_mb(self) -> float:
        """Peak memory in MB."""
        return self.memory_peak / (1024 * 1024)


class TRProfiler:
    """Profiler for transreal operations."""
    
    def __init__(self, trace_memory: bool = True):
        """
        Initialize profiler.
        
        Args:
            trace_memory: Whether to trace memory allocations
        """
        self.trace_memory = trace_memory
        self._results: Dict[str, ProfileResult] = {}
        self._stack: List[ProfileResult] = []
        self._lock = threading.Lock()
        self._enabled = False
    
    def __enter__(self):
        """Enter profiling context."""
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit profiling context."""
        self.stop()
    
    def start(self):
        """Start profiling."""
        with self._lock:
            self._enabled = True
            if self.trace_memory:
                tracemalloc.start()
    
    def stop(self):
        """Stop profiling."""
        with self._lock:
            self._enabled = False
            if self.trace_memory:
                tracemalloc.stop()
    
    def profile_operation(self, name: str):
        """
        Decorator to profile an operation.
        
        Args:
            name: Name of the operation
        """
        def decorator(func: Callable) -> Callable:
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                if not self._enabled:
                    return func(*args, **kwargs)
                
                # Start timing
                start_time = time.perf_counter()
                
                # Memory snapshot
                if self.trace_memory:
                    snapshot_start = tracemalloc.take_snapshot()
                
                # Create result object
                result = ProfileResult(
                    name=name,
                    duration=0.0,
                    calls=1,
                    memory_allocated=0,
                    memory_peak=0
                )
                
                # Push to stack
                with self._lock:
                    parent = self._stack[-1] if self._stack else None
                    self._stack.append(result)
                
                try:
                    # Execute function
                    output = func(*args, **kwargs)
                    
                    # Analyze output for tag distribution
                    try:
                        if isinstance(output, TRNode):
                            tag_name = output.value.tag.name
                            result.tag_distribution[tag_name] = result.tag_distribution.get(tag_name, 0) + 1
                        elif isinstance(output, TRScalar):
                            tag_name = output.tag.name
                            result.tag_distribution[tag_name] = result.tag_distribution.get(tag_name, 0) + 1
                    except Exception:
                        pass
                    
                    return output
                
                finally:
                    # Stop timing
                    end_time = time.perf_counter()
                    result.duration = end_time - start_time
                    
                    # Memory analysis
                    if self.trace_memory:
                        snapshot_end = tracemalloc.take_snapshot()
                        stats = snapshot_end.compare_to(snapshot_start, 'lineno')
                        
                        total_allocated = sum(stat.size_diff for stat in stats if stat.size_diff > 0)
                        result.memory_allocated = total_allocated
                        
                        current, peak = tracemalloc.get_traced_memory()
                        result.memory_peak = peak
                    
                    # Pop from stack and update parent
                    with self._lock:
                        self._stack.pop()
                        if parent:
                            parent.sub_operations.append(result)
                        else:
                            # Top-level operation
                            if name in self._results:
                                # Merge with existing
                                existing = self._results[name]
                                existing.duration += result.duration
                                existing.calls += 1
                                existing.memory_allocated += result.memory_allocated
                                existing.memory_peak = max(existing.memory_peak, result.memory_peak)
                                for tag, count in result.tag_distribution.items():
                                    existing.tag_distribution[tag] = existing.tag_distribution.get(tag, 0) + count
                            else:
                                self._results[name] = result
            
            return wrapper
        return decorator
    
    def get_results(self) -> Dict[str, ProfileResult]:
        """Get profiling results."""
        with self._lock:
            return dict(self._results)
    
    def generate_report(self) -> str:
        """Generate a human-readable profiling report."""
        results = self.get_results()
        if not results:
            return "No profiling data collected."
        
        lines = ["Transreal Profiling Report", "=" * 50, ""]
        
        # Sort by total time
        sorted_results = sorted(results.values(), key=lambda r: r.duration, reverse=True)
        
        # Summary table
        lines.append(f"{'Operation':<30} {'Calls':>10} {'Total(s)':>12} {'Avg(ms)':>12} {'Memory(MB)':>12}")
        lines.append("-" * 90)
        
        for result in sorted_results:
            lines.append(
                f"{result.name:<30} {result.calls:>10} "
                f"{result.duration:>12.4f} {result.avg_duration*1000:>12.2f} "
                f"{result.memory_allocated_mb:>12.2f}"
            )
        
        # Tag distribution
        lines.extend(["", "Tag Distribution:", "-" * 30])
        total_tags = defaultdict(int)
        for result in sorted_results:
            for tag, count in result.tag_distribution.items():
                total_tags[tag] += count
        
        for tag, count in sorted(total_tags.items()):
            lines.append(f"{tag:<15} {count:>10}")
        
        return "\n".join(lines)


# Global profiler instance
_global_profiler = TRProfiler()


def profile_tr_operation(name: Optional[str] = None):
    """
    Decorator to profile a transreal operation.
    
    Args:
        name: Name of the operation (uses function name if None)
    """
    def decorator(func: Callable) -> Callable:
        op_name = name or func.__name__
        return _global_profiler.profile_operation(op_name)(func)
    
    return decorator


def memory_profile(func: Callable) -> Callable:
    """
    Decorator to profile memory usage of a function.
    
    Args:
        func: Function to profile
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        gc.collect()
        
        if not tracemalloc.is_tracing():
            tracemalloc.start()
        
        snapshot_before = tracemalloc.take_snapshot()
        start_current, start_peak = tracemalloc.get_traced_memory()
        
        try:
            result = func(*args, **kwargs)
            
            snapshot_after = tracemalloc.take_snapshot()
            end_current, end_peak = tracemalloc.get_traced_memory()
            
            # Compute statistics
            stats = snapshot_after.compare_to(snapshot_before, 'lineno')
            
            print(f"\nMemory Profile for {func.__name__}:")
            print(f"  Current memory: {end_current / 1024 / 1024:.2f} MB")
            print(f"  Peak memory: {end_peak / 1024 / 1024:.2f} MB")
            print(f"  Memory allocated: {(end_current - start_current) / 1024 / 1024:.2f} MB")
            
            # Top allocations
            print("\n  Top memory allocations:")
            for stat in stats[:5]:
                print(f"    {stat}")
            
            return result
            
        finally:
            tracemalloc.stop()
    
    return wrapper


def tag_statistics(nodes: List[TRNode]) -> Dict[str, Any]:
    """
    Compute tag statistics for a list of nodes.
    
    Args:
        nodes: List of TR nodes
        
    Returns:
        Dictionary with tag statistics
    """
    stats = {
        'total': len(nodes),
        'by_tag': defaultdict(int),
        'by_operation': defaultdict(lambda: defaultdict(int)),
        'non_real_operations': [],
    }
    
    for node in nodes:
        # Support both TRNode and TRScalar inputs in stats
        if hasattr(node, 'value') and hasattr(node.value, 'tag'):
            tag = node.value.tag.name
        elif hasattr(node, 'tag'):
            tag = node.tag.name
        else:
            # Unknown object, skip
            continue
        stats['by_tag'][tag] += 1
        
        if hasattr(node, '_grad_info') and node._grad_info:
            op_type = node._grad_info.op_type.name
            stats['by_operation'][op_type][tag] += 1
            
            if tag != 'REAL':
                value_str = str(node.value) if hasattr(node, 'value') else ''
                stats['non_real_operations'].append({
                    'operation': op_type,
                    'tag': tag,
                    'value': value_str,
                })
    
    # Compute percentages
    total = stats['total']
    stats['percentages'] = {
        tag: (count / total * 100) if total > 0 else 0
        for tag, count in stats['by_tag'].items()
    }
    
    return dict(stats)


def performance_report(graph_root: TRNode) -> Dict[str, Any]:
    """
    Generate a comprehensive performance report for a computational graph.
    
    Args:
        graph_root: Root node of the graph
        
    Returns:
        Performance report dictionary
    """
    report = {
        'graph_statistics': {},
        'memory_analysis': {},
        'optimization_potential': {},
        'bottlenecks': [],
    }
    
    # Graph statistics
    nodes = _collect_all_nodes(graph_root)
    report['graph_statistics'] = {
        'total_nodes': len(nodes),
        'depth': _compute_graph_depth(graph_root),
        'branching_factor': _compute_branching_factor(nodes),
    }
    
    # Tag statistics
    report['tag_statistics'] = tag_statistics(nodes)
    
    # Memory analysis
    node_size = sys.getsizeof(TRNode.constant(TRScalar(0.0, TRTag.REAL)))
    total_memory = len(nodes) * node_size
    
    report['memory_analysis'] = {
        'estimated_memory_mb': total_memory / (1024 * 1024),
        'nodes_per_mb': (1024 * 1024) / node_size if node_size > 0 else 0,
    }
    
    # Optimization potential
    redundant_ops = _find_redundant_operations(nodes)
    fusable_chains = _find_fusable_chains(nodes)
    
    report['optimization_potential'] = {
        'redundant_operations': len(redundant_ops),
        'fusable_chains': len(fusable_chains),
        'estimated_speedup': f"{(1 + 0.1 * len(redundant_ops) + 0.2 * len(fusable_chains)):.1f}x",
    }
    
    # Identify bottlenecks
    report['bottlenecks'] = _identify_bottlenecks(nodes)
    
    return report


def _collect_all_nodes(root: TRNode) -> List[TRNode]:
    """Collect all nodes in a graph."""
    visited = set()
    nodes = []
    stack = [root]
    
    while stack:
        node = stack.pop()
        if id(node) in visited:
            continue
        
        visited.add(id(node))
        nodes.append(node)
        
        if node._grad_info and node._grad_info.inputs:
            for inp_ref in node._grad_info.inputs:
                inp = inp_ref()
                if inp is not None:
                    stack.append(inp)
    
    return nodes


def _compute_graph_depth(root: TRNode) -> int:
    """Compute the depth of a computational graph."""
    def depth_recursive(node: TRNode, visited: set) -> int:
        if id(node) in visited:
            return 0
        
        visited.add(id(node))
        
        if not node._grad_info or not node._grad_info.inputs:
            return 1
        
        max_depth = 0
        for inp_ref in node._grad_info.inputs:
            inp = inp_ref()
            if inp is not None:
                max_depth = max(max_depth, depth_recursive(inp, visited))
        
        return max_depth + 1
    
    return depth_recursive(root, set())


def _compute_branching_factor(nodes: List[TRNode]) -> float:
    """Compute average branching factor."""
    total_branches = 0
    nodes_with_branches = 0
    
    for node in nodes:
        if node._grad_info and node._grad_info.inputs:
            num_inputs = len([inp for inp in node._grad_info.inputs if inp() is not None])
            if num_inputs > 0:
                total_branches += num_inputs
                nodes_with_branches += 1
    
    return total_branches / nodes_with_branches if nodes_with_branches > 0 else 0.0


def _find_redundant_operations(nodes: List[TRNode]) -> List[Tuple[TRNode, TRNode]]:
    """Find potentially redundant operations."""
    redundant = []
    
    # Group by operation type and value
    groups = defaultdict(list)
    for node in nodes:
        if node._grad_info:
            key = (node._grad_info.op_type, node.value.value, node.value.tag)
            groups[key].append(node)
    
    # Find groups with multiple nodes
    for key, group_nodes in groups.items():
        if len(group_nodes) > 1:
            for i in range(1, len(group_nodes)):
                redundant.append((group_nodes[0], group_nodes[i]))
    
    return redundant


def _find_fusable_chains(nodes: List[TRNode]) -> List[List[TRNode]]:
    """Find chains of operations that could be fused."""
    chains = []
    
    # Simple pattern: chains of adds or muls
    for node in nodes:
        if node._grad_info and node._grad_info.op_type in [OpType.ADD, OpType.MUL]:
            chain = [node]
            current = node
            
            # Follow chain of same operation
            while current._grad_info and len(current._grad_info.inputs) > 0:
                next_ref = current._grad_info.inputs[0]
                next_node = next_ref() if next_ref else None
                
                if (next_node and next_node._grad_info and 
                    next_node._grad_info.op_type == node._grad_info.op_type):
                    chain.append(next_node)
                    current = next_node
                else:
                    break
            
            if len(chain) > 2:
                chains.append(chain)
    
    return chains


def _identify_bottlenecks(nodes: List[TRNode]) -> List[Dict[str, Any]]:
    """Identify potential performance bottlenecks."""
    bottlenecks = []
    
    # High fan-out nodes (used by many others)
    usage_count = defaultdict(int)
    for node in nodes:
        if node._grad_info and node._grad_info.inputs:
            for inp_ref in node._grad_info.inputs:
                inp = inp_ref()
                if inp is not None:
                    usage_count[id(inp)] += 1
    
    for node in nodes:
        count = usage_count.get(id(node), 0)
        if count > 10:  # Arbitrary threshold
            bottlenecks.append({
                'type': 'high_fanout',
                'description': f'Node used by {count} other nodes',
                'severity': 'medium' if count < 50 else 'high',
                'node_info': {
                    'operation': node._grad_info.op_type.name if node._grad_info else 'constant',
                    'tag': node.value.tag.name,
                }
            })
    
    # Non-REAL heavy operations
    non_real_ops = defaultdict(int)
    for node in nodes:
        if node.value.tag != TRTag.REAL and node._grad_info:
            non_real_ops[node._grad_info.op_type.name] += 1
    
    for op_name, count in non_real_ops.items():
        if count > 5:  # Arbitrary threshold
            bottlenecks.append({
                'type': 'non_real_operations',
                'description': f'{count} {op_name} operations producing non-REAL values',
                'severity': 'low' if count < 20 else 'medium',
            })
    
    return bottlenecks


# Convenience function for quick profiling
def quick_profile(func: Callable, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """
    Quick profile a function call.
    
    Args:
        func: Function to profile
        *args: Positional arguments
        **kwargs: Keyword arguments
        
    Returns:
        Tuple of (function result, profile data)
    """
    with TRProfiler() as profiler:
        wrapped = profiler.profile_operation(func.__name__)(func)
        result = wrapped(*args, **kwargs)
        
        profile_data = {
            'results': profiler.get_results(),
            'report': profiler.generate_report(),
        }
        
    return result, profile_data
